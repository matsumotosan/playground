{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9fcff5",
   "metadata": {},
   "source": [
    "# SwapVAE\n",
    "\n",
    "[SwapVAE](https://proceedings.neurips.cc/paper/2021/file/58182b82110146887c02dbd78719e3d5-Paper.pdf) is a modification of a vanilla VAE architecture that allows for the partitioning of the latent representation into \"content\" and \"style\" components. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb028651",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329353db-ec1f-48f1-96bd-63f146101268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02336b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from pytorch_lightning import Trainer\n",
    "from playground.models import SwapVAE\n",
    "from playground.datamodules import MNISTDataModule\n",
    "from playground.utils import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbfab5-fdf7-42c1-a99b-f45e49c5b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '../lightning_logs'\n",
    "pretrained_pth = '../pretrained'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c39981",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d02279-5d8c-4979-9445-bbeb7edfb95e",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c50fd92-0d66-4794-a001-e2c7c86de16a",
   "metadata": {},
   "source": [
    "As mentioned before, SwapVAE is similar to that of a vanilla VAE. The novelty  of the architecture is in the `BlockSwap` operation, as coined by the authors, which swaps the content component of the latent representations of two samples. In their schematic, this corresponds to a change in reach direction whilst preserving the reach dynamic.\n",
    "\n",
    "![swapvae](img/swapvae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da779b-e6c9-464e-ba36-abcb7581e1ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22efc87-df64-45b7-9277-d25377029940",
   "metadata": {
    "tags": []
   },
   "source": [
    "The loss function is composed of three components:\n",
    "- Reconstruction loss $\\mathcal{L}_{\\text{rec}}$\n",
    "- Style space regularization $\\mathcal{D}_{KL}$\n",
    "- Content space alignment loss $\\mathcal{L}_{\\text{align}}$\n",
    "\n",
    "The style space regularization and content space alignment loss are weighted by $\\beta$ and $\\alpha$, respectively.\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{f,g} \\sum_{i=1,2} \\mathcal{L}_{\\text{rec}} (\\mathbf{x}_i, g(\\mathbf{z}_i)) + \\beta\\sum_{i=1,2} \\mathcal{D}_{KL} (\\mathbf{z}_i^{(s)} || \\mathbf{z}_{i,\\text{prior}}^{(s)}) + \\alpha\\mathcal{L}_{\\text{align}} (\\mathbf{z}_1^{(c)}, \\mathbf{z}_2^{(c)})\n",
    "\\label{loss_total}\n",
    "\\end{equation}\n",
    "\n",
    "To promote disentanglement of the content and style bilaterally, the reconstruction loss is further refined to consider the swapped representations as well:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\text{align}}^{\\text{swap}} = \\mathcal{L}_{\\text{rec}} (\\mathbf{x}_i, g(\\tilde{\\mathbf{z}}_i)) + \\mathcal{L}_{\\text{rec}} (\\mathbf{x}_i, g(\\mathbf{z}_i))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca7e91-6d44-4abd-ab9f-a8effc7a184b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03c290-d4b4-4116-8fab-ce121e31cf66",
   "metadata": {},
   "source": [
    "The model is trained using the following hyperparameters:\n",
    "- `learning_rate`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55a3bd-d09e-4fcd-8000-526c04001fc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892ff27-e852-4263-ac78-76edd761185e",
   "metadata": {},
   "source": [
    "The SwapVAE architecture consists of an encoder and decoder, much like a vanilla VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578b851-7098-435d-ba37-b95bfe9e48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 256\n",
    "hidden_dim = [128, 64, 64]\n",
    "content_dim = 16\n",
    "style_dim = 16\n",
    "\n",
    "model = SwapVAE(input_dim, hidden_dim, content_dim, style_dim)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42797012-0d84-48f3-9ef8-dca77433e923",
   "metadata": {},
   "source": [
    "We can pass in a pair of 256-dimensional vectors $x_1$ and $x_2$ to confirm that it works. The outputs $y_1$ and $y_2$ should be vectors of the same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa13da-7094-42e5-8734-fb5ea71e2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.rand(input_dim)\n",
    "x2 = torch.rand(input_dim)\n",
    "\n",
    "y1, y2 = model(x1, x2)\n",
    "\n",
    "y1.shape, y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faca71c-3ae6-4927-8fec-537a99c0ed3d",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df035c25-71c6-4200-90a7-a6a630843c71",
   "metadata": {},
   "source": [
    "The authors conducted experiments on synthetic and real datasets on neural data; however, as with many demonstrations of models, we will be using MNIST as well as SVHN, another dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068d448-7f81-401f-b8c8-16af1c7df626",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523283c-55da-4760-a08f-2d7b1edc1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "batch_size = 32\n",
    "num_workers = 8\n",
    "train_val_split = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42036a47-82ea-45c5-a508-96ec177765df",
   "metadata": {},
   "source": [
    "Initialize a PyTorch LightningDataModule for the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44825d-fe46-4a78-9fa9-072351b3ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dm = MNISTDataModule(\n",
    "    data_dir=data_dir,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    split=train_val_split\n",
    ")\n",
    "\n",
    "mnist_dm.prepare_data()\n",
    "mnist_dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6adc6f-a701-4502-8a93-855276f35511",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_dm.train_dataloader()\n",
    "inputs, classes = next(iter(mnist_train))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc35bb0-7452-4636-b369-ebd1e4799a9d",
   "metadata": {},
   "source": [
    "### SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d8b74-d7f0-4144-84b9-ce42478e3472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7439d776-b7df-42ed-b6b1-822f4438b113",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964e040-1450-4184-ac70-4b78a4cbfa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epochs = 1\n",
    "max_epochs = 1\n",
    "\n",
    "trainer = Trainer(min_epochs=min_epochs, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4071fd-8c12-4ce4-836b-79d86e7021c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, mnist_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ed03b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db60e43d-36db-4735-ba15-c42b6b66e22b",
   "metadata": {},
   "source": [
    "[Drop, Swap, and Generate: A Self-Supervised Approach for Generating Neural Activity](https://proceedings.neurips.cc/paper/2021/file/58182b82110146887c02dbd78719e3d5-Paper.pdf)\n",
    "\n",
    "[PyTorch Lightning](https://www.pytorchlightning.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:play] *",
   "language": "python",
   "name": "conda-env-play-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e23238229963226bf4fce80318fdf7cd570226175462aeffd64567312f546a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
